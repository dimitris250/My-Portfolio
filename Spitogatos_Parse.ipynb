{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a web crawler for the site spitogatos.gr. It is a property listing site in greece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chage this link based on the area you want to select\n",
    "url='https://www.spitogatos.gr/search/results/residential/rent/r100/m100m/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Headers are used in order to \"trick\" the website that the user is not a bot\n",
    "headers={}\n",
    "headers['User-Agent']='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'\n",
    "link= unquote(url)\n",
    "#    print(link)\n",
    "text= requests.get(link, headers=headers).text\n",
    "soup= bs.BeautifulSoup(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for this particular website the user needs to select the specified area he want in order for listings to show up\n",
    "# Hypothetically we can make a list of all the links for the areas and loop through the entire website but I chose to do it in\n",
    "# way so it does not take a long time to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this section the program take the specified number of listings for the particular area so the user only need to change \n",
    "# the initial link and the rest will be automated.\n",
    "num_of_listing= soup.find('h2',class_=\"padding-left h5 searchTotalNumberOfResults\").parent.find_next('b')\n",
    "print(num_of_listing.text.strip())\n",
    "lenght= int(float(num_of_listing.text.strip())*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Makes a list of all the possible pages that this area has\n",
    "page_links=[]\n",
    "for number in range(0,lenght,10):\n",
    "        #print(number)\n",
    "        url_pages = url + 'offset_' + str(number)\n",
    "        page_links.append(url_pages)1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finds the links to all the posts and appends it into a dataframe. I used a dataframe because I ran through some error using lists\n",
    "posts=[]\n",
    "for pages in page_links:\n",
    "    req1= unquote(pages)\n",
    "    resp1 = requests.get(req1,headers=headers).text\n",
    "    soup1 = bs.BeautifulSoup(resp1,'html.parser')\n",
    "    for post in soup1.find_all('div',class_=\"bd padding-right\"):\n",
    "        post= post.a.get('href')\n",
    "        posts.append(post)\n",
    "\n",
    "df = pd.DataFrame(columns = ['link'],data=posts)\n",
    "\n",
    "df.link.str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This last part opens each post and scrapes the price, price per m^2, size in m^2, type of property, number of rooms, number of baths\n",
    "# what floop the property is in, the parking availability, the year the property was built, the last update of the listing and \n",
    "# finally the specific area the property is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prices=[]\n",
    "pricesper=[]\n",
    "sizes=[]\n",
    "kinds=[]\n",
    "heat_types=[]\n",
    "num_of_rooms=[]\n",
    "num_of_baths=[]\n",
    "floors=[]\n",
    "parking=[]\n",
    "year_builts=[]\n",
    "last_update=[]\n",
    "area=[]\n",
    "for i in range(len(df)):\n",
    "    link = (df.link.iloc[i])\n",
    "    link= unquote(link)\n",
    "#    print(link)\n",
    "    texts= requests.get(link, headers=headers).text\n",
    "    soup2= bs.BeautifulSoup(texts)\n",
    "    \n",
    "    try:\n",
    "        price= soup2.find('h6',string='Τιμή').parent.find_next('div') \n",
    "        prices.append(price.text.strip())\n",
    "    except AttributeError:\n",
    "        prices.append('None')\n",
    "   \n",
    "    try:\n",
    "        priceper= soup2.find('h6', string='Τιμή ανά τ.μ.').parent.find_next('div')\n",
    "        pricesper.append(priceper.text.strip())\n",
    "    except AttributeError:\n",
    "        pricesper.append('None')\n",
    "    \n",
    "    try:    \n",
    "        size= soup2.find('h6', string='Εμβαδό').parent.find_next('div')\n",
    "        sizes.append(size.text.strip())\n",
    "    except AttributeError:\n",
    "        sizes.append('None')\n",
    "    \n",
    "    try:\n",
    "        kind= soup2.find('h6', string='Τύπος').parent.find_next('div')\n",
    "        kinds.append(kind.text.strip())\n",
    "    except AttributeError:\n",
    "        kinds.append('None')\n",
    "        \n",
    "    try:\n",
    "        num_of_room= soup2.find('h6', string='Υπνοδωμάτια').parent.find_next('div')\n",
    "        num_of_rooms.append(num_of_room.text.strip())\n",
    "    except AttributeError:\n",
    "        num_of_rooms.append('None')\n",
    "    \n",
    "    try:\n",
    "        num_of_bath= soup2.find('h6', string='Μπάνια').parent.find_next('div')\n",
    "        num_of_baths.append(num_of_bath.text.strip())\n",
    "    except AttributeError:\n",
    "        num_of_baths.append('None')\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        floor= soup2.find('h6', string='Όροφος').parent.find_next('div')\n",
    "        floors.append(floor.text.strip())\n",
    "    except AttributeError:\n",
    "        floors.append('None')\n",
    "    \n",
    "    try:\n",
    "        park= soup2.find('h6', string='Θέση στάθμευσης').parent.find_next('div')\n",
    "        parking.append(park.text.strip())\n",
    "    except AttributeError:\n",
    "        parking.append('None')\n",
    "    \n",
    "    try:\n",
    "        year_buitl= soup2.find('h6', string='Έτος κατασκευής').parent.find_next('div')\n",
    "        year_builts.append(year_buitl.text.strip())\n",
    "    except AttributeError:\n",
    "        year_builts.append('None')\n",
    "        \n",
    "    try:\n",
    "        last_updates= soup2.find('h6',string='Τελευταία ενημέρωση').parent.find_next('div')\n",
    "        last_update.append(last_updates.text.strip())\n",
    "    except AttributeError:\n",
    "        last_update.append('None')\n",
    "    \n",
    "    try:\n",
    "        areas= soup2.find('h6',string='Περιοχή').parent.find_next('div')\n",
    "        area.append(areas.text.strip())\n",
    "    except AttributeError:\n",
    "        area.append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally we append everything into a dataframe for further analysis and save it into a file that the user must change everytime\n",
    "# a new area is being screaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_info= pd.DataFrame(\n",
    "                    {'prices':prices,\n",
    "                     'priceper':pricesper,\n",
    "                     'size':sizes,\n",
    "                     'kind':kinds, \n",
    "                     'num_of_rooms':num_of_rooms,\n",
    "                     'num_of_bath':num_of_baths,\n",
    "                     'floor':floors,\n",
    "                     'parking':parking,\n",
    "                     'year_buitl':year_builts,\n",
    "                     'last_update':last_update,\n",
    "                     'area':area})    \n",
    "    \n",
    "unique_info.to_csv('spitogatos_center.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
